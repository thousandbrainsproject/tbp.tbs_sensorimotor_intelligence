# Copyright 2025 Thousand Brains Project
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.
import argparse
import os
import shutil
import subprocess as sp
import sys
from pathlib import Path

description = """
Download and extract datasets used by `tbp.tbs_sensorimotor_intelligence`.

This script downloads datasets required to run experiments and/or plotting
scripts. It accepts one or more dataset names as arguments.

Usage:
    python download.py [dataset1] [dataset2] ...

Available datasets:
  ycb                     The YCB object dataset (~500 MB). Required for
                          running experiments using `tbp.monty`. See
                          https://www.ycbbenchmarks.com for details.

  pretrained_models
                          Pretrained models used by `tbp.monty` in this
                          paper (3.4 GB). Required for running experiments
                          (unless recomputed locally).

  results                 Output generated by running core experiments in
                          the paper (2.5 GB). Required for running plotting
                          scripts (unless recomputed locally).

  visualizations          Results from experiments used for generating
                          some visualizations for figures (~850 MB).
                          Required for running plotting scripts (unless
                          recomputed locally).

  view_finder_images
                          Intermediate output used to generate input data
                          for the ViT model (400 MB). Not required for
                          running Monty experiments or plotting scripts.

Notes:
  - Two environment variables determine where files are downloaded to.
    - `MONTY_DATA`: Path where the YCB dataset will be downloaded.
                    Default: `~/tbp/data`
    - `DMC_ROOT_DIR`: Path where all other datasets will be downloaded.
                      Default: `~/tbp/results/dmc`
  - If a requested dataset already exists, you will be prompted before it
    is overwritten. To force overwrite without prompting, use the `-f/--force` flag.

"""

parser = argparse.ArgumentParser(
    description=description,
    formatter_class=argparse.RawDescriptionHelpFormatter,
)
parser.add_argument(
    "datasets",
    nargs="+",
    help="One or more datasets to download (e.g. pretrained_models)",
)
parser.add_argument(
    "-f",
    "--force",
    action="store_true",
    default=False,
    help="Automatically overwrite existing datasets without prompting.",
)

# Environment variables.
DMC_ROOT_DIR = Path(os.environ.get("DMC_ROOT_DIR", "~/tbp/results/dmc")).expanduser()
MONTY_DATA = Path(os.environ.get("MONTY_DATA", "~/tbp/data")).expanduser()

# DMC datasets.
DMC_DATASETS = {
    "pretrained_models": {
        "url": "https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.tbs_sensorimotor_intelligence/monty/pretrained_models.tgz",
        "destination.compressed": DMC_ROOT_DIR / "pretrained_models.tgz",
        "destination.uncompressed": DMC_ROOT_DIR / "pretrained_models",
    },
    "results": {
        "url": "https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.tbs_sensorimotor_intelligence/monty/results.tgz",
        "destination.compressed": DMC_ROOT_DIR / "results.tgz",
        "destination.uncompressed": DMC_ROOT_DIR / "results",
    },
    "visualizations": {
        "url": "https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.tbs_sensorimotor_intelligence/monty/visualizations.tgz",
        "destination.compressed": DMC_ROOT_DIR / "visualizations.tgz",
        "destination.uncompressed": DMC_ROOT_DIR / "visualizations",
    },
    "view_finder_images": {
        "url": "https://tbp-pretrained-models-public-c9c24aef2e49b897.s3.us-east-2.amazonaws.com/tbp.tbs_sensorimotor_intelligence/monty/view_finder_images.tgz",
        "destination.compressed": DMC_ROOT_DIR / "view_finder_images.tgz",
        "destination.uncompressed": DMC_ROOT_DIR / "view_finder_images",
    },
}


def confirm_overwrite(dataset: str, path: Path, force: bool = False) -> bool:
    """Ask the user if it should overwrite an existing file.

    Args:
        path: An existing path.

    Returns:
        True if the user confirms overwrite, False otherwise.
    """
    if force:
        return True
    prompt = f"{dataset} dataset already exists at '{path}'. Overwrite? (y/[n]): "
    while True:
        choice = input(prompt).strip().lower()
        if choice == "y":
            return True
        elif choice in ("n", ""):
            return False


def download_dmc_dataset(dataset: str, force: bool = False) -> None:
    """Download and extract one of our datasets.

    Args:
        dataset: The name of the dataset to download (e.g. "pretrained_models").
    """
    dataset_info = DMC_DATASETS[dataset]

    # Check if the (uncompressed) destination already exists. If so, ask the user
    # if they want to overwrite it.
    if dataset_info["destination.uncompressed"].exists():
        overwrite = confirm_overwrite(
            dataset, dataset_info["destination.uncompressed"], force=force
        )
        if overwrite:
            # Delete the destination.
            if dataset_info["destination.uncompressed"].is_dir():
                shutil.rmtree(dataset_info["destination.uncompressed"])
            else:
                dataset_info["destination.uncompressed"].unlink()
        else:
            return

    # Download the compressed file.
    print(f"Downloading {dataset} dataset...")
    dataset_info["destination.compressed"].parent.mkdir(parents=True, exist_ok=True)
    command = [
        "curl",
        "-L",
        dataset_info["url"],
        "-o",
        dataset_info["destination.compressed"],
    ]
    try:
        sp.run(command, check=True)
    except Exception as e:
        print(f"[ERROR] Failed to download {dataset}: {e}")
        sys.exit(1)

    # Decompress the file.
    command = [
        "tar",
        "-xzf",
        dataset_info["destination.compressed"],
        "-C",
        dataset_info["destination.uncompressed"].parent,
    ]
    try:
        sp.run(command, check=True)
    except Exception as e:
        print(f"[ERROR] Failed to extract {dataset}: {e}")
        sys.exit(1)

    # Delete the compressed file.
    dataset_info["destination.compressed"].unlink()


def download_ycb_dataset(force: bool = False) -> None:
    """Download the YCB object dataset."""
    ycb_data_dir = MONTY_DATA / "habitat"
    if ycb_data_dir.exists():
        overwrite = confirm_overwrite("ycb", ycb_data_dir, force=force)
        if overwrite:
            shutil.rmtree(ycb_data_dir)
        else:
            return
    print("Downloading ycb dataset...")
    command = [
        "python",
        "-m",
        "habitat_sim.utils.datasets_download",
        "--uids",
        "ycb",
        "--data-path",
        ycb_data_dir,
    ]
    try:
        sp.run(command, check=True)
    except Exception as e:
        print(f"[ERROR] Failed to download the YCB dataset: {e}")
        sys.exit(1)


def main() -> None:
    """Download and extract datasets specified via command line arguments.

    This function serves as the entry point for the download script.
    """
    args = parser.parse_args()

    datasets = list(args.datasets)
    force = args.force

    # Validate the arguments.
    for name in datasets:
        if not (name in DMC_DATASETS or name == "ycb"):
            print(f"Invalid dataset '{name}'.")
            sys.exit(1)

    # Download and extract datasets.
    for name in datasets:
        if name == "ycb":
            download_ycb_dataset(force=force)
        else:
            download_dmc_dataset(name, force=force)


if __name__ == "__main__":
    main()
