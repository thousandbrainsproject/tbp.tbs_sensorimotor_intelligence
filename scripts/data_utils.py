# Copyright 2025 Thousand Brains Project
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.
"""
Data I/O, paths, and other utilities.
"""

import json
import os
from copy import deepcopy
from pathlib import Path
from typing import Any, Container, Iterable, Mapping, Optional, Union

import numpy as np
import pandas as pd
import torch
from numpy.typing import ArrayLike
from scipy.spatial.transform import Rotation as R

# Path settings - mirrors those in configs/common.py
DMC_ROOT_DIR = Path(os.environ.get("DMC_ROOT_DIR", "~/tbp/results/dmc")).expanduser()
DMC_PRETRAIN_DIR = DMC_ROOT_DIR / "pretrained_models"
DMC_RESULTS_DIR = DMC_ROOT_DIR / "results"
VISUALIZATION_RESULTS_DIR = DMC_ROOT_DIR / "visualizations"

# Root directory for output generated by figure scripts.
DMC_ANALYSIS_DIR = Path(
    os.environ.get("DMC_ANALYSIS_DIR", "~/tbp/results/dmc_analysis")
).expanduser()
DMC_ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)


def load_eval_stats(exp: os.PathLike) -> pd.DataFrame:
    """Load `eval_stats.csv` files.

    This function has 3 main purposes:
     - Load `eval_stats.csv` given just a DMC experiment name since this function
       is aware of DMC result paths.
     - Convert strings of arrays into arrays. For example, some columns contain
       arrays, but they're loaded as strings (e.g., "[1.34, 232.33, 123.44]").
     - Add some useful columns to the dataframe (`"episode"`, `"epoch"`).

    Args:
        exp (os.PathLike): Name of a DMC experiment, a directory containing
          `eval_stats.csv`, or a complete path to an `.csv` file.

    NOTE: For added DMC context and motivation, some analyses use rotations from this
    table, such as symmetry analysis in figure 3 where rotation errors are computed.
    For this, the the string-represented rotations in the loaded table needed to be
    converted to numpy arrays. It seemed cleaner to perform that that task here rather
    than do it in analysis scripts. Similarly, the 'episode' column is useful to have
    for downstream analyses.


    Returns:
        pd.DataFrame
    """
    path = Path(exp).expanduser()

    if path.exists():
        # Case 1: Given a path to a csv file.
        if path.suffix.lower() == ".csv":
            df = pd.read_csv(exp)
        # Case 2: Given a path to a directory containing eval_stats.csv.
        elif (path / "eval_stats.csv").exists():
            df = pd.read_csv(path / "eval_stats.csv")
        else:
            raise FileNotFoundError(f"No eval_stats.csv found for {exp}")
    else:
        # Given a run name. Look in DMC folder.
        df = pd.read_csv(DMC_RESULTS_DIR / path / "eval_stats.csv")

    # Remove redundant first column (which just has LM IDs)
    if df.columns[0] == "Unnamed: 0":
        df = df.iloc[:, 1:]

    # Collect basic info, like number of LMs, objects, number of episodes, etc.
    n_lms = len(np.unique(df["lm_id"]))
    object_names = np.unique(df["primary_target_object"])
    n_objects = len(object_names)

    # Add 'episode' column.
    assert len(df) % n_lms == 0  # sanity check
    n_episodes = int(len(df) / n_lms)
    df["episode"] = np.repeat(np.arange(n_episodes), n_lms)

    # Add 'epoch' column.
    rows_per_epoch = n_objects * n_lms
    assert len(df) % rows_per_epoch == 0  # sanity check
    n_epochs = int(len(df) / rows_per_epoch)
    df["epoch"] = np.repeat(np.arange(n_epochs), rows_per_epoch)

    # Decode array columns.
    def decode_arrays(s: Any, dtype: type) -> Any:
        """Converts a string-represented array to a numpy array.

        NOTE: if the input is not a string-representation of a list/tuple/array, it is
        returned as-is.
        """
        if not isinstance(s, str):
            return s

        # Quick out for empty strings.
        if s == "":
            return ""

        # Remove the outer brackets and parentheses. If it doesn't have
        # brackets or parentheses, it's not an array, so just return it as-is.
        if s.startswith("["):
            s = s.strip("[]")
        elif s.startswith("("):
            s = s.strip("()")
        else:
            return s

        # Split the string into a list of elements.
        if "," in s:
            # list and tuples are comma-separated
            lst = [elt.strip() for elt in s.split(",")]
        else:
            # numpy arrays are space-separated
            lst = s.split()

        # arrays of strings are a special case - can return arrays with dtype 'object',
        # and we also need to strip quotes from each item.
        if np.issubdtype(dtype, np.str_):
            lst = [elt.strip("'\"") for elt in lst]
            if dtype is str:
                return np.array(lst, dtype=object)
            else:
                return np.array(lst, dtype=dtype)

        # Must replace 'None' with np.nan for float arrays.
        if np.issubdtype(dtype, np.floating):
            lst = [np.nan if elt == "None" else dtype(elt) for elt in lst]
        return np.array(lst)

    float_array_cols = [
        "primary_target_position",
        "primary_target_rotation_euler",
        "most_likely_rotation",
        "detected_location",
        "detected_rotation",
        "location_rel_body",
        "detected_path",
        "most_likely_rotation",
        "primary_target_rotation_quat",
    ]
    column_order = list(df.columns)
    df["result"] = df["result"].replace(np.nan, "")
    df["result"] = df["result"].apply(decode_arrays, args=[str])
    for col_name in float_array_cols:
        df[col_name] = df[col_name].apply(decode_arrays, args=[float])
    df = df[column_order]
    return df


def load_floppy_traces(exp: os.PathLike) -> pd.DataFrame:
    """Load and process floppy experiment statistics.

    This function reads flop traces from a floppy experiment directory
    and returns a DataFrame with statistics about flops.

    Args:
        exp (os.PathLike): Name of a floppy experiment or path to experiment directory.

    Returns:
        pd.DataFrame: DataFrame containing experiment statistics with columns:
            - experiment: Name of the experiment
            - flops_mean: Mean flops per episode
            - flops_std: Standard deviation of flops per episode
    """
    path = Path(exp).expanduser()
    if not path.exists():
        # Given a run name. Look in DMC folder.
        path = DMC_RESULTS_DIR / path

    # Initialize results dictionary
    result_dict = {
        "experiment": [path.name],
        "flops_mean": [np.nan],
        "flops_std": [np.nan],
    }

    # Read all flop traces files
    experiment_flops = []
    files = list(path.glob("flop_traces*.csv"))

    for file in files:
        flops_df = pd.read_csv(file)
        # Get average of flops for experiment.run_episode in method column
        run_episode_df = flops_df[flops_df["method"] == "experiment.run_episode"]
        experiment_flops.extend(run_episode_df["flops"].tolist())

    # Calculate flops statistics
    if experiment_flops:
        result_dict["flops_mean"] = [np.mean(experiment_flops)]
        result_dict["flops_std"] = [np.std(experiment_flops)]

    return pd.DataFrame(result_dict)


def load_perf_stat_flops(exp: os.PathLike) -> pd.DataFrame:
    """Load and process performance statistics data from perf stat output.

    This function reads performance statistics data from perf stat output files
    and returns a DataFrame with the total FLOPs.

    The input file should be in the format:
    # started on [timestamp]
    count,,instruction_type,percentage,value,42.00,,

    Args:
        exp (os.PathLike): CSV file containing perf stat output.

    Returns:
        pd.DataFrame: DataFrame containing experiment statistics with columns:
            - experiment: Name of the experiment
            - flops_mean: Total number of floating point operations
    """
    path = Path(exp).expanduser()
    if not path.exists():
        raise FileNotFoundError(f"No perf stat output file found for {exp}")

    # Read the perf stat output file
    # Skip the header line with timestamp
    df = pd.read_csv(path, comment="#", header=None)

    # Rename columns based on the data format
    df.columns = [
        "count",
        "empty1",
        "instruction_type",
        "percentage",
        "value",
        "empty2",
        "empty3",
        "empty4",
    ]

    # Clean up the data
    df = df.drop(columns=["empty1", "empty2", "empty3", "empty4"])
    df["percentage"] = df["percentage"].str.rstrip("%").astype(float)

    # Define FLOPs per operation for different instruction types
    flops_per_operation = {
        "fp_arith_inst_retired.128b_packed_double": 2,
        "fp_arith_inst_retired.128b_packed_single": 4,
        "fp_arith_inst_retired.256b_packed_double": 4,
        "fp_arith_inst_retired.256b_packed_single": 8,
        "fp_arith_inst_retired.512b_packed_double": 8,
        "fp_arith_inst_retired.512b_packed_single": 16,
        "fp_arith_inst_retired.scalar_double": 1,
        "fp_arith_inst_retired.scalar_single": 1,
    }

    # Calculate total FLOPs
    total_flops = sum(
        row["count"] * flops_per_operation.get(row["instruction_type"], 0)
        for _, row in df.iterrows()
        if row["count"] > 0 and row["instruction_type"] in flops_per_operation
    )

    return pd.DataFrame({"experiment": [path.name], "flops_mean": [total_flops]})


def load_vit_predictions(path: os.PathLike) -> pd.DataFrame:
    """Load and process ViT model predictions from a CSV file.

    This function reads a CSV file containing ViT model predictions with real and 
    predicted class labels and quaternions, and returns a DataFrame with processed data.

    Args:
        path (os.PathLike): Path to the CSV file containing ViT predictions.

    Returns:
        pd.DataFrame: DataFrame containing:
            - real_class: The true class label
            - predicted_class: The predicted class label
            - real_quaternion: The true quaternion as a numpy array
            - predicted_quaternion: The predicted quaternion as a numpy array
            - quaternion_error_degs: The quaternion error in degrees
    """
    # Read the CSV file
    df = pd.read_csv(path)

    # Convert string quaternions to numpy arrays
    def parse_quaternion(q_str):
        # Remove brackets and split by comma
        q_str = q_str.strip("[]")
        return np.array([float(x) for x in q_str.split(",")])

    # Apply parsing to quaternion columns
    df["real_quaternion"] = df["real_quaternion"].apply(parse_quaternion)
    df["predicted_quaternion"] = df["predicted_quaternion"].apply(parse_quaternion)

    return df


def get_frequency(items: Iterable, match: Union[Any, Container[Any]]) -> float:
    """Get the fraction of values that belong to a collection of values.

    Args:
        items (iterable): The data to count.
        match: (scalar or list of scalars): One or more values to match against
          (e.g., `"correct"` or `["correct", "correct_mlh"]`).
    Returns:
        float: The frequency that values in `items` belong to `match`.
    """
    s = items if isinstance(items, pd.Series) else pd.Series(items)
    match = np.atleast_1d(match)
    value_counts = dict(s.value_counts())
    n_matching = sum([value_counts.get(val, 0) for val in match])
    return n_matching / len(s)


def aggregate_1lm_performance_data(experiments: Iterable[str]) -> pd.DataFrame:
    """Save the performance table for the single LM experiments.

    Output is saved to `DMC_ANALYSIS_DIR/fig3/performance/single_lm_performance.csv`.
    """

    columns = {
        "accuracy": [],
        "percent.correct": [],
        "percent.correct_mlh": [],
        "n_steps": [],
        "n_steps.mean": [],
        "n_steps.median": [],
        "rotation_error": [],
        "rotation_error.mean": [],
        "rotation_error.median": [],
    }
    for exp in experiments:
        eval_stats = load_eval_stats(exp)
        accuracy = 100 * get_frequency(
            eval_stats["primary_performance"], ("correct", "correct_mlh")
        )
        percent_correct = 100 * get_frequency(
            eval_stats["primary_performance"], "correct"
        )
        percent_correct_mlh = 100 * get_frequency(
            eval_stats["primary_performance"], "correct_mlh"
        )
        n_steps = eval_stats["num_steps"]
        rotation_error = np.degrees(eval_stats.rotation_error.dropna())

        columns["accuracy"].append(accuracy)
        columns["percent.correct"].append(percent_correct)
        columns["percent.correct_mlh"].append(percent_correct_mlh)

        columns["n_steps"].append(n_steps)
        columns["n_steps.mean"].append(n_steps.mean())
        columns["n_steps.median"].append(n_steps.median())

        columns["rotation_error"].append(rotation_error)
        columns["rotation_error.mean"].append(rotation_error.mean())
        columns["rotation_error.median"].append(rotation_error.median())

    return pd.DataFrame(columns, index=experiments)


class DetailedJSONStatsInterface:
    """Convenience interface to detailed JSON stats.

    This convenience interface to detailed JSON stats files. It's primarily useful for
    efficiently iterating over episodes.

    Example:
        >>> stats = DetailedJSONStatsInterface("detailed_stats.json")
        >>> last_episode_data = stats[-1]  # Get data for the last episode.
        >>> # Iterate over all episodes. Faster than loading individual episodes
        >>> # via random access.
        >>> for i, episode_data in enumerate(stats):
        ...     # Do something with episode data.
        ...     pass
    """

    def __init__(self, path: os.PathLike):
        self._path = Path(path)
        self._index = None  # Just used to convert possibly negative indices

    @property
    def path(self) -> os.PathLike:
        return self._path

    def read_episode(self, episode: int) -> Mapping:
        self._check_initialized()
        assert np.isscalar(episode)
        episode = self._index[episode]
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                if i == episode:
                    return list(json.loads(line).values())[0]

    def _check_initialized(self):
        if self._index is not None:
            return
        length = 0
        with open(self._path, "r") as f:
            length = sum(1 for _ in f)
        self._index = np.arange(length)

    def __iter__(self):
        with open(self._path, "r") as f:
            for i, line in enumerate(f):
                yield list(json.loads(line).values())[0]

    def __len__(self) -> int:
        self._check_initialized()
        return len(self._index)

    def __getitem__(self, episode: int) -> dict:
        """Get the stats for a given episode.

        Args:
            episode (int): The episode number.

        Returns:
            dict: The stats for the episode.
        """
        return self.read_episode(episode)


class ObjectModel:
    """Mutable wrapper for object models.

    Args:
        pos (ArrayLike): The points of the object model as a sequence of points
          (i.e., has shape (n_points, 3)).
        features (Optional[Mapping]): The features of the object model. For
          convenience, the features become attributes of the ObjectModel instance.
    """

    def __init__(
        self,
        pos: ArrayLike,
        features: Optional[Mapping[str, ArrayLike]] = None,
    ):
        self.pos = np.asarray(pos, dtype=float)
        if features:
            for key, value in features.items():
                setattr(self, key, np.asarray(value))

    @property
    def x(self) -> np.ndarray:
        return self.pos[:, 0]

    @property
    def y(self) -> np.ndarray:
        return self.pos[:, 1]

    @property
    def z(self) -> np.ndarray:
        return self.pos[:, 2]

    def copy(self, deep: bool = True) -> "ObjectModel":
        return deepcopy(self) if deep else self

    def rotated(
        self,
        rotation: Union[R, ArrayLike],
        degrees: bool = False,
    ) -> "ObjectModel":
        """Rotate the object model.

        Args:
            rotation: Rotation to apply. May be one of
              - A `scipy.spatial.transform.Rotation` object.
              - A 3x3 rotation matrix.
              - A 3-element array of x, y, z euler angles.
            degrees (bool): Whether Euler angles are in degrees. Ignored
                if `rotation` is not a 1D array.

        Returns:
            ObjectModel: The rotated object model.
        """
        if isinstance(rotation, R):
            rot = rotation
        else:
            arr = np.asarray(rotation)
            if arr.shape == (3,):
                rot = R.from_euler("xyz", arr, degrees=degrees)
            elif arr.shape == (3, 3):
                rot = R.from_matrix(arr)
            else:
                raise ValueError(f"Invalid rotation argument: {rotation}")

        pos = rot.apply(self.pos)
        out = self.copy()
        out.pos = pos

        return out

    def __add__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        out = deepcopy(self)
        out.pos += translation
        return out

    def __sub__(self, translation: ArrayLike) -> "ObjectModel":
        translation = np.asarray(translation)
        return self + (-translation)


def load_object_model(
    model_name: str,
    object_name: str,
    features: Optional[Iterable[str]] = ("rgba",),
    checkpoint: Optional[int] = None,
    lm_id: int = 0,
) -> ObjectModel:
    """Load an object model from a pretraining experiment.

    Args:
        model_name (str): The name of the model to load (e.g., `dist_agent_1lm`).
        object_name (str): The name of the object to load (e.g., `mug`).
        checkpoint (Optional[int]): The checkpoint to load. Defaults to None. Most
          pretraining experiments aren't checkpointed, so this is usually None.
        lm_id (int): The ID of the LM to load. Defaults to 0.

    Returns:
        ObjectModel: The loaded object model.

    Example:
        >>> model = load_object_model("dist_agent_1lm", "mug")
        >>> model -= [0, 1.5, 0]
        >>> rotation = R.from_euler("xyz", [0, 90, 0], degrees=True)
        >>> rotated = model.rotated(rotation)
        >>> print(model.rgba.shape)
        (1354, 4)
    """
    if checkpoint is None:
        model_path = DMC_PRETRAIN_DIR / model_name / "pretrained/model.pt"
    else:
        model_path = (
            DMC_PRETRAIN_DIR
            / model_name
            / f"pretrained/checkpoints/{checkpoint}/model.pt"
        )
    data = torch.load(model_path)
    data = data["lm_dict"][lm_id]["graph_memory"][object_name]["patch"]
    points = np.array(data.pos, dtype=float)
    if features:
        features = [features] if isinstance(features, str) else features
        feature_dict = {}
        for feature in features:
            if feature not in data.feature_mapping:
                print(f"WARNING: Feature {feature} not found in data.feature_mapping")
                continue
            idx = data.feature_mapping[feature]
            feature_data = np.array(data.x[:, idx[0] : idx[1]])
            if feature == "rgba":
                feature_data = feature_data / 255.0
            feature_dict[feature] = feature_data

    return ObjectModel(points, features=feature_dict)
