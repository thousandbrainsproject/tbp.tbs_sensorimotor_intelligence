# @package _global_

# Copyright 2025 Thousand Brains Project
#
# Copyright may exist in Contributors' modifications
# and/or contributions to the work.
#
# Use of this source code is governed by the MIT
# license that can be found in the LICENSE file or at
# https://opensource.org/licenses/MIT.

defaults:
  - /experiment/hyperparameter_optimization/starting_config
  - _self_

model:
  rotation_weight: 1 
  net: # Added following 05_multilayer_heads experiment
    model_name: vit-b16-224-in21k
    num_classes: 77
    freeze_backbone: false
    use_pretrained: true
  optimizer: # Added following 07_adamw_optimizer experiment
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0005
    weight_decay: 0.01
    betas: [0.9, 0.999] # default betas for AdamW
    eps: 1e-8 # default eps for AdamW
  scheduler: # Added following 06_warmup_cosine_decay experiment
    _target_: transformers.get_cosine_schedule_with_warmup
    _partial_: true

trainer:
  gradient_clip_val: 1.0 # Added following 02_gradient_clipping experiment
