model_checkpoint:
  _target_: lightning.pytorch.callbacks.ModelCheckpoint
  dirpath: null # directory to save the model file
  filename: "epoch_{epoch:03d}-val_loss={val/loss:.3f}" # checkpoint filename with metrics
  monitor: null # name of the logged metric which determines when model is improving
  verbose: False # verbosity mode
  save_last: true # additionally always save an exact copy of the last checkpoint
  save_top_k: 3 # save k best models (determined by monitor metric)
  mode: "min" # "max" means higher metric value is better, can be also "min"
  auto_insert_metric_name: true # when True, the checkpoints filenames will contain the metric name
  save_weights_only: false # if True, then only the model's weights will be saved
  every_n_train_steps: null # number of training steps between checkpoints
  train_time_interval: null # checkpoints are monitored at the specified time interval
  every_n_epochs: 1 # number of epochs between checkpoints
  save_on_train_epoch_end: null # whether to run checkpointing at the end of training epoch
